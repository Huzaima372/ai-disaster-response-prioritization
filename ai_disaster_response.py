# -*- coding: utf-8 -*-
"""AI_Disaster_Response.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QgglWV9nJWGUD0RjXxQC5JxUDiZOhglX
"""

# !pip install tensorflow transformers scikit-learn opencv-python matplotlib pandas numpy kaggle

import kagglehub

# Download latest version
path = kagglehub.dataset_download("tunguz/xview2-challenge-dataset-train-and-test")

print("Path to dataset files:", path)

import os
import json
import cv2
import numpy as np

IMG_SIZE = 224

def load_xview2_data(image_dir, label_dir, limit=300):
    images = []
    labels = []

    image_files = os.listdir(image_dir)[:limit]

    for img_file in image_files:
        img_path = os.path.join(image_dir, img_file)
        label_path = os.path.join(label_dir, img_file.replace(".png", ".json"))

        if not os.path.exists(label_path):
            continue

        # Load image
        img = cv2.imread(img_path)
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        img = img / 255.0

        # Load label
        with open(label_path) as f:
            data = json.load(f)

        damaged = 0
        for feat in data["features"]["xy"]:
            subtype = feat["properties"]["subtype"]
            if subtype != "no-damage":
                damaged = 1
                break

        images.append(img)
        labels.append(damaged)

    return np.array(images), np.array(labels)

import os
import json
import cv2
import numpy as np

IMG_SIZE = 224

def load_xview2_data(image_dir, label_dir, limit=300):
    images = []
    labels = []

    image_files = os.listdir(image_dir)[:limit]

    for img_file in image_files:
        img_path = os.path.join(image_dir, img_file)
        label_path = os.path.join(label_dir, img_file.replace(".png", ".json"))

        if not os.path.exists(label_path):
            continue

        # Load image
        img = cv2.imread(img_path)
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        img = img / 255.0

        # Load label
        with open(label_path) as f:
            data = json.load(f)

        damaged = 0
        # Add checks for 'features' and 'xy' keys
        if "features" in data and "xy" in data["features"]:
            for feat in data["features"]["xy"]:
                # Safely get 'properties' and then 'subtype'
                properties = feat.get("properties", {})
                subtype = properties.get("subtype", "no-damage") # Default to "no-damage" if 'subtype' is missing

                if subtype != "no-damage":
                    damaged = 1
                    break

        images.append(img)
        labels.append(damaged)

    return np.array(images), np.array(labels)

X_train, y_train = load_xview2_data(train_images_path, train_labels_path)
X_val, y_val = load_xview2_data(train_images_path, train_labels_path)

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

base_model = ResNet50(
    weights="imagenet",
    include_top=False,
    input_shape=(224,224,3)
)

base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation="relu")(x)
output = Dense(1, activation="sigmoid")(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

model.summary()



history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=8
)

model.save("satellite_damage_model.h5")

def priority_score(damage_prob, tweet_prob):
    score = 0.6 * damage_prob + 0.4 * tweet_prob

    if score > 0.7:
        return "ðŸ”´ HIGH PRIORITY"
    elif score > 0.4:
        return "ðŸŸ  MEDIUM PRIORITY"
    else:
        return "ðŸŸ¢ LOW PRIORITY"

tweet_urgency = 0.75  # from NLP model
print(priority_score(damage_prob, tweet_urgency))

tweet_urgency = 0.75  # from NLP model
print(priority_score(damage_prob, tweet_urgency))



# /////////////////////////////////////////////////////

import kagglehub

# Download latest version
path = kagglehub.dataset_download("vbmokin/nlp-with-disaster-tweets-cleaning-data")

print("Path to dataset files:", path)

import os
os.listdir(path)

import pandas as pd
df = pd.read_csv(os.path.join(path, "train_data_cleaning2.csv"))
df.head()

df[['text', 'target']].sample(5)

from sklearn.model_selection import train_test_split

X_train_nlp, X_val_nlp, y_train_nlp, y_val_nlp = train_test_split(
    df['text'],
    df['target'],
    test_size=0.2,
    random_state=42,
    stratify=df['target']
)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=8000,
    ngram_range=(1,2),
    stop_words="english"
)

X_train_vec = vectorizer.fit_transform(X_train_nlp)
X_val_vec = vectorizer.transform(X_val_nlp)

from sklearn.linear_model import LogisticRegression

tweet_model = LogisticRegression(
    max_iter=1000,
    class_weight="balanced"
)

tweet_model.fit(X_train_vec, y_train_nlp)

val_acc_nlp = tweet_model.score(X_val_vec, y_val_nlp)
print("Tweet Urgency Validation Accuracy:", val_acc_nlp)

import pickle

with open("tweet_urgency_model.pkl", "wb") as f:
    pickle.dump(tweet_model, f)

with open("tfidf_vectorizer.pkl", "wb") as f:
    pickle.dump(vectorizer, f)

def predict_tweet_urgency(text):
    vec = vectorizer.transform([text])
    prob = tweet_model.predict_proba(vec)[0][1]
    return prob

sample_tweets = [
    "People are trapped under buildings, urgent rescue needed!",
    "Lovely weather today, enjoying my coffee"
]

for t in sample_tweets:
    print(t)
    print("Urgency Probability:", predict_tweet_urgency(t))
    print("-----")

damage_prob = 0.81  # output from ResNet50 model
tweet_prob = predict_tweet_urgency(
    "Major flooding, families stranded on rooftops!"
)

def priority_score(damage_prob, tweet_prob):
    score = 0.6 * damage_prob + 0.4 * tweet_prob

    if score > 0.7:
        return "ðŸ”´ HIGH PRIORITY"
    elif score > 0.4:
        return "ðŸŸ  MEDIUM PRIORITY"
    else:
        return "ðŸŸ¢ LOW PRIORITY"

print("FINAL PRIORITY:", priority_score(damage_prob, tweet_prob))


"""STEP 1: Create Sample Disaster Zones (Demo Data)"""

import pandas as pd

data = {
    "Zone": ["Zone A", "Zone B", "Zone C", "Zone D"],
    "Damage_Prob": [0.85, 0.40, 0.70, 0.20],
    "Tweet_Urgency_Prob": [0.90, 0.35, 0.60, 0.10]
}

df = pd.DataFrame(data)
df

"""STEP 2: Apply Priority Scoring Function"""

def compute_priority(row):
    score = 0.6 * row["Damage_Prob"] + 0.4 * row["Tweet_Urgency_Prob"]
    if score > 0.7:
        return "HIGH"
    elif score > 0.4:
        return "MEDIUM"
    else:
        return "LOW"

df["Priority"] = df.apply(compute_priority, axis=1)
df

"""STEP 3: Priority Score Visualization (Bar Chart)"""

import matplotlib.pyplot as plt

df["Priority_Score"] = 0.6 * df["Damage_Prob"] + 0.4 * df["Tweet_Urgency_Prob"]

plt.figure(figsize=(8,5))
plt.bar(df["Zone"], df["Priority_Score"])
plt.axhline(0.7, linestyle="--")
plt.axhline(0.4, linestyle="--")
plt.title("Disaster Response Priority Scores")
plt.ylabel("Priority Score")
plt.xlabel("Affected Zones")
plt.show()

"""STEP 4: (OPTIONAL) Color-Coded Priority Table"""

def color_priority(val):
    if val == "HIGH":
        return "background-color: #ff4d4d"
    elif val == "MEDIUM":
        return "background-color: #ffd966"
    else:
        return "background-color: #8fd19e"

df.style.applymap(color_priority, subset=["Priority"])

